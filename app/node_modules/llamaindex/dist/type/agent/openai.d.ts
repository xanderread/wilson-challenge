import { OpenAIAgent } from "@llamaindex/openai";
export declare const OpenAIContextAwareAgent: {
    new (params: import("@llamaindex/core/agent").LLMAgentParams & import("./contextAwareMixin.js").ContextAwareConfig): {
        readonly contextRetriever: import("@llamaindex/core/retriever").BaseRetriever;
        retrievedContext: string | null;
        chatHistory: import("@llamaindex/core/llms").ChatMessage<object>[];
        retrieveContext(query: import("@llamaindex/core/llms").MessageContent): Promise<string>;
        injectContext(context: string): Promise<void>;
        chat(params: import("@llamaindex/core/chat-engine").NonStreamingChatEngineParams): Promise<import("@llamaindex/core/schema").EngineResponse>;
        chat(params: import("@llamaindex/core/chat-engine").StreamingChatEngineParams): Promise<ReadableStream<import("@llamaindex/core/schema").EngineResponse>>;
    };
    defaultCreateStore(): object;
    defaultTaskHandler: import("@llamaindex/core/agent").TaskHandler<import("@llamaindex/core/llms").LLM>;
    shouldContinue<AI extends import("@llamaindex/core/llms").LLM, Store extends object = {}, AdditionalMessageOptions extends object = AI extends import("@llamaindex/core/llms").LLM<object, infer AdditionalMessageOptions_1 extends object> ? AdditionalMessageOptions_1 : never>(task: Readonly<import("@llamaindex/core/agent").TaskStep<AI, Store, AdditionalMessageOptions>>): boolean;
} & typeof OpenAIAgent;
export type { ContextAwareConfig } from "./contextAwareMixin.js";
export * from "@llamaindex/openai";
