import { AnthropicAgent, type AnthropicAgentParams } from "@llamaindex/anthropic";
import type { NonStreamingChatEngineParams, StreamingChatEngineParams } from "@llamaindex/core/chat-engine";
import type { MessageContent } from "@llamaindex/core/llms";
import type { BaseRetriever } from "@llamaindex/core/retriever";
import { EngineResponse } from "@llamaindex/core/schema";
import { OpenAIAgent, type OpenAIAgentParams } from "@llamaindex/openai";
export interface ContextAwareConfig {
    contextRetriever: BaseRetriever;
}
export interface ContextAwareState {
    contextRetriever: BaseRetriever;
    retrievedContext: string | null;
}
export type SupportedAgent = typeof OpenAIAgent | typeof AnthropicAgent;
export type AgentParams<T> = T extends typeof OpenAIAgent ? OpenAIAgentParams : T extends typeof AnthropicAgent ? AnthropicAgentParams : never;
/**
 * ContextAwareAgentRunner enhances the base AgentRunner with the ability to retrieve and inject relevant context
 * for each query. This allows the agent to access and utilize appropriate information from a given index or retriever,
 * providing more informed and context-specific responses to user queries.
 */
export declare function withContextAwareness<T extends SupportedAgent>(Base: T): {
    new (params: AgentParams<T> & ContextAwareConfig): {
        readonly contextRetriever: BaseRetriever;
        retrievedContext: string | null;
        chatHistory: T extends typeof OpenAIAgent ? OpenAIAgent["chatHistory"] : T extends typeof AnthropicAgent ? AnthropicAgent["chatHistory"] : never;
        retrieveContext(query: MessageContent): Promise<string>;
        injectContext(context: string): Promise<void>;
        chat(params: NonStreamingChatEngineParams): Promise<EngineResponse>;
        chat(params: StreamingChatEngineParams): Promise<ReadableStream<EngineResponse>>;
    };
    defaultCreateStore(): object;
    defaultTaskHandler: import("@llamaindex/core/agent").TaskHandler<import("@llamaindex/core/llms").LLM>;
    shouldContinue<AI extends import("@llamaindex/core/llms").LLM, Store extends object = {}, AdditionalMessageOptions extends object = AI extends import("@llamaindex/core/llms").LLM<object, infer AdditionalMessageOptions_1 extends object> ? AdditionalMessageOptions_1 : never>(task: Readonly<import("@llamaindex/core/agent").TaskStep<AI, Store, AdditionalMessageOptions>>): boolean;
} & T;
