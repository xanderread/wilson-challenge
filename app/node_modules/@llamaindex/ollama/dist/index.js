import { BaseEmbedding } from '@llamaindex/core/embeddings';
import { extractText, streamConverter } from '@llamaindex/core/utils';
import { Ollama as Ollama$1 } from 'ollama/browser';

const messageAccessor = (part)=>{
    return {
        raw: part,
        delta: part.message.content
    };
};
const completionAccessor = (part)=>{
    return {
        text: part.response,
        raw: part
    };
};
class Ollama extends BaseEmbedding {
    constructor(params){
        super(), this.options = {
            num_ctx: 4096,
            top_p: 0.9,
            temperature: 0.7
        };
        this.model = params.model;
        this.ollama = new Ollama$1(params.config);
        if (params.options) {
            this.options = {
                ...this.options,
                ...params.options
            };
        }
    }
    get metadata() {
        const { temperature, top_p, num_ctx } = this.options;
        return {
            model: this.model,
            temperature: temperature,
            topP: top_p,
            maxTokens: this.options.num_ctx,
            contextWindow: num_ctx,
            tokenizer: undefined
        };
    }
    async chat(params) {
        const { messages, stream } = params;
        const payload = {
            model: this.model,
            messages: messages.map((message)=>({
                    role: message.role,
                    content: extractText(message.content)
                })),
            stream: !!stream,
            options: {
                ...this.options
            }
        };
        if (!stream) {
            const chatResponse = await this.ollama.chat({
                ...payload,
                stream: false
            });
            return {
                message: {
                    role: "assistant",
                    content: chatResponse.message.content
                },
                raw: chatResponse
            };
        } else {
            const stream = await this.ollama.chat({
                ...payload,
                stream: true
            });
            return streamConverter(stream, messageAccessor);
        }
    }
    async complete(params) {
        const { prompt, stream } = params;
        const payload = {
            model: this.model,
            prompt: extractText(prompt),
            stream: !!stream,
            options: {
                ...this.options
            }
        };
        if (!stream) {
            const response = await this.ollama.generate({
                ...payload,
                stream: false
            });
            return {
                text: response.response,
                raw: response
            };
        } else {
            const stream = await this.ollama.generate({
                ...payload,
                stream: true
            });
            return streamConverter(stream, completionAccessor);
        }
    }
    async getEmbedding(prompt) {
        const payload = {
            model: this.model,
            prompt,
            options: {
                ...this.options
            }
        };
        const response = await this.ollama.embeddings({
            ...payload
        });
        return response.embedding;
    }
    async getTextEmbedding(text) {
        return this.getEmbedding(text);
    }
}

export { Ollama };
