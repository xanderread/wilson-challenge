import { BaseEmbedding } from '@llamaindex/core/embeddings';
import { LLM, LLMMetadata, LLMChatParamsStreaming, ChatResponseChunk, LLMChatParamsNonStreaming, ChatResponse, LLMCompletionParamsStreaming, CompletionResponse, LLMCompletionParamsNonStreaming } from '@llamaindex/core/llms';
import { Config, Options, Ollama as Ollama$1 } from 'ollama/browser';

type OllamaParams = {
    model: string;
    config?: Partial<Config>;
    options?: Partial<Options>;
};
declare class Ollama extends BaseEmbedding implements LLM {
    readonly ollama: Ollama$1;
    model: string;
    options: Partial<Omit<Options, "num_ctx" | "top_p" | "temperature">> & Pick<Options, "num_ctx" | "top_p" | "temperature">;
    constructor(params: OllamaParams);
    get metadata(): LLMMetadata;
    chat(params: LLMChatParamsStreaming): Promise<AsyncIterable<ChatResponseChunk>>;
    chat(params: LLMChatParamsNonStreaming): Promise<ChatResponse>;
    complete(params: LLMCompletionParamsStreaming): Promise<AsyncIterable<CompletionResponse>>;
    complete(params: LLMCompletionParamsNonStreaming): Promise<CompletionResponse>;
    private getEmbedding;
    getTextEmbedding(text: string): Promise<number[]>;
}

export { Ollama, type OllamaParams };
