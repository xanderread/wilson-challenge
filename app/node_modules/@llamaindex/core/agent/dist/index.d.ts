import { ChatMessage, MessageContent, LLM, BaseToolWithCall, ToolOutput, ChatResponse, ChatResponseChunk, BaseTool, ToolCall, PartialToolCall, TextChatMessage, ToolCallLLMMessageOptions } from '../../llms/dist/index.js';
import { BaseMemory } from '../../memory/dist/index.js';
import { EngineResponse } from '../../schema/dist/index.js';
import { ObjectRetriever } from '../../objects/dist/index.js';
import { Logger } from '@llamaindex/env';
import { UUID } from '../../global/dist/index.js';

interface BaseChatEngineParams<AdditionalMessageOptions extends object = object> {
    message: MessageContent;
    /**
     * Optional chat history if you want to customize the chat history.
     */
    chatHistory?: ChatMessage<AdditionalMessageOptions>[] | BaseMemory<AdditionalMessageOptions>;
}
interface StreamingChatEngineParams<AdditionalMessageOptions extends object = object> extends BaseChatEngineParams<AdditionalMessageOptions> {
    stream: true;
}
interface NonStreamingChatEngineParams<AdditionalMessageOptions extends object = object> extends BaseChatEngineParams<AdditionalMessageOptions> {
    stream?: false;
}
declare abstract class BaseChatEngine {
    abstract chat(params: NonStreamingChatEngineParams): Promise<EngineResponse>;
    abstract chat(params: StreamingChatEngineParams): Promise<AsyncIterable<EngineResponse>>;
    abstract chatHistory: ChatMessage[] | Promise<ChatMessage[]>;
}

type AgentTaskContext<Model extends LLM, Store extends object = {}, AdditionalMessageOptions extends object = Model extends LLM<object, infer AdditionalMessageOptions> ? AdditionalMessageOptions : never> = {
    readonly stream: boolean;
    readonly toolCallCount: number;
    readonly llm: Model;
    readonly getTools: (input: MessageContent) => BaseToolWithCall[] | Promise<BaseToolWithCall[]>;
    shouldContinue: (taskStep: Readonly<TaskStep<Model, Store, AdditionalMessageOptions>>) => boolean;
    store: {
        toolOutputs: ToolOutput[];
        messages: ChatMessage<AdditionalMessageOptions>[];
    } & Store;
    logger: Readonly<Logger>;
};
type TaskStep<Model extends LLM = LLM, Store extends object = {}, AdditionalMessageOptions extends object = Model extends LLM<object, infer AdditionalMessageOptions> ? AdditionalMessageOptions : never> = {
    id: UUID;
    context: AgentTaskContext<Model, Store, AdditionalMessageOptions>;
    prevStep: TaskStep<Model, Store, AdditionalMessageOptions> | null;
    nextSteps: Set<TaskStep<Model, Store, AdditionalMessageOptions>>;
};
type TaskStepOutput<Model extends LLM, Store extends object = {}, AdditionalMessageOptions extends object = Model extends LLM<object, infer AdditionalMessageOptions> ? AdditionalMessageOptions : never> = {
    taskStep: TaskStep<Model, Store, AdditionalMessageOptions>;
    output: ChatResponse<AdditionalMessageOptions> | ReadableStream<ChatResponseChunk<AdditionalMessageOptions>>;
    isLast: boolean;
};
type TaskHandler<Model extends LLM, Store extends object = {}, AdditionalMessageOptions extends object = Model extends LLM<object, infer AdditionalMessageOptions> ? AdditionalMessageOptions : never> = (step: TaskStep<Model, Store, AdditionalMessageOptions>, enqueueOutput: (taskOutput: TaskStepOutput<Model, Store, AdditionalMessageOptions>) => void) => Promise<void>;
type AgentStartEvent = {
    startStep: TaskStep;
};
type AgentEndEvent = {
    endStep: TaskStep;
};

type AgentRunnerParams<AI extends LLM, Store extends object = {}, AdditionalMessageOptions extends object = AI extends LLM<object, infer AdditionalMessageOptions> ? AdditionalMessageOptions : never> = {
    llm: AI;
    chatHistory: ChatMessage<AdditionalMessageOptions>[];
    systemPrompt: MessageContent | null;
    runner: AgentWorker<AI, Store, AdditionalMessageOptions>;
    tools: BaseToolWithCall[] | ((query: MessageContent) => Promise<BaseToolWithCall[]>);
    verbose: boolean;
};
type AgentParamsBase<AI extends LLM, AdditionalMessageOptions extends object = AI extends LLM<object, infer AdditionalMessageOptions> ? AdditionalMessageOptions : never> = {
    llm?: AI;
    chatHistory?: ChatMessage<AdditionalMessageOptions>[];
    systemPrompt?: MessageContent;
    verbose?: boolean;
    tools: BaseToolWithCall[];
} | {
    llm?: AI;
    chatHistory?: ChatMessage<AdditionalMessageOptions>[];
    systemPrompt?: MessageContent;
    verbose?: boolean;
    toolRetriever: ObjectRetriever<BaseToolWithCall>;
};
/**
 * Worker will schedule tasks and handle the task execution
 */
declare abstract class AgentWorker<AI extends LLM, Store extends object = {}, AdditionalMessageOptions extends object = AI extends LLM<object, infer AdditionalMessageOptions> ? AdditionalMessageOptions : never> {
    #private;
    abstract taskHandler: TaskHandler<AI, Store, AdditionalMessageOptions>;
    createTask(query: MessageContent, context: AgentTaskContext<AI, Store, AdditionalMessageOptions>): ReadableStream<TaskStepOutput<AI, Store, AdditionalMessageOptions>>;
    [Symbol.toStringTag]: string;
}
/**
 * Runner will manage the task execution and provide a high-level API for the user
 */
declare abstract class AgentRunner<AI extends LLM, Store extends object = {}, AdditionalMessageOptions extends object = AI extends LLM<object, infer AdditionalMessageOptions> ? AdditionalMessageOptions : never> extends BaseChatEngine {
    #private;
    abstract createStore(): Store;
    static defaultCreateStore(): object;
    static defaultTaskHandler: TaskHandler<LLM>;
    protected constructor(params: AgentRunnerParams<AI, Store, AdditionalMessageOptions>);
    get llm(): AI;
    get chatHistory(): ChatMessage<AdditionalMessageOptions>[];
    get verbose(): boolean;
    reset(): void;
    getTools(query: MessageContent): Promise<BaseToolWithCall[]> | BaseToolWithCall[];
    static shouldContinue<AI extends LLM, Store extends object = {}, AdditionalMessageOptions extends object = AI extends LLM<object, infer AdditionalMessageOptions> ? AdditionalMessageOptions : never>(task: Readonly<TaskStep<AI, Store, AdditionalMessageOptions>>): boolean;
    createTask(message: MessageContent, stream?: boolean, verbose?: boolean | undefined, chatHistory?: ChatMessage<AdditionalMessageOptions>[]): ReadableStream<TaskStepOutput<AI, Store, AdditionalMessageOptions>>;
    chat(params: NonStreamingChatEngineParams): Promise<EngineResponse>;
    chat(params: StreamingChatEngineParams): Promise<ReadableStream<EngineResponse>>;
}

type LLMParamsBase = AgentParamsBase<LLM>;
type LLMParamsWithTools = LLMParamsBase & {
    tools: BaseToolWithCall[];
};
type LLMParamsWithToolRetriever = LLMParamsBase & {
    toolRetriever: ObjectRetriever<BaseToolWithCall>;
};
type LLMAgentParams = LLMParamsWithTools | LLMParamsWithToolRetriever;
declare class LLMAgentWorker extends AgentWorker<LLM> {
    taskHandler: TaskHandler<LLM<object, object>>;
}
declare class LLMAgent extends AgentRunner<LLM> {
    constructor(params: LLMAgentParams);
    createStore: typeof AgentRunner.defaultCreateStore;
    taskHandler: TaskHandler<LLM<object, object>>;
}

type StepToolsResponseParams<Model extends LLM> = {
    response: ChatResponse<ToolCallLLMMessageOptions>;
    tools: BaseTool[];
    step: Parameters<TaskHandler<Model, {}, ToolCallLLMMessageOptions>>[0];
    enqueueOutput: Parameters<TaskHandler<Model, {}, ToolCallLLMMessageOptions>>[1];
};
type StepToolsStreamingResponseParams<Model extends LLM> = Omit<StepToolsResponseParams<Model>, "response"> & {
    response: AsyncIterable<ChatResponseChunk<ToolCallLLMMessageOptions>>;
};
declare function stepToolsStreaming<Model extends LLM>({ response, tools, step, enqueueOutput, }: StepToolsStreamingResponseParams<Model>): Promise<void>;
declare function stepTools<Model extends LLM>({ response, tools, step, enqueueOutput, }: StepToolsResponseParams<Model>): Promise<void>;
declare function callTool(tool: BaseTool | undefined, toolCall: ToolCall | PartialToolCall, logger: Logger): Promise<ToolOutput>;
declare function consumeAsyncIterable<Options extends object>(input: ChatMessage<Options>, previousContent?: string): Promise<ChatMessage<Options>>;
declare function consumeAsyncIterable<Options extends object>(input: AsyncIterable<ChatResponseChunk<Options>>, previousContent?: string): Promise<TextChatMessage<Options>>;
declare function createReadableStream<T>(asyncIterable: AsyncIterable<T>): ReadableStream<T>;
declare function validateAgentParams<AI extends LLM>(params: AgentParamsBase<AI>): void;

export { type AgentEndEvent, type AgentParamsBase, AgentRunner, type AgentStartEvent, AgentWorker, LLMAgent, type LLMAgentParams, LLMAgentWorker, type TaskHandler, type TaskStep, callTool, consumeAsyncIterable, createReadableStream, stepTools, stepToolsStreaming, validateAgentParams };
