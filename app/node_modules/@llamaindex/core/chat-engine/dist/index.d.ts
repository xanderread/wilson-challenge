import * as ___llms from '../../llms/dist/index.js';
import { MessageContent, ChatMessage, LLM, MessageType } from '../../llms/dist/index.js';
import { BaseMemory } from '../../memory/dist/index.js';
import { EngineResponse, NodeWithScore, MetadataMode } from '../../schema/dist/index.js';
import { BaseNodePostprocessor } from '../../postprocessor/dist/index.js';
import { PromptMixin, ContextSystemPrompt, PromptsRecord, ModuleRecord } from '../../prompts/dist/index.js';
import { BaseRetriever } from '../../retriever/dist/index.js';

interface BaseChatEngineParams<AdditionalMessageOptions extends object = object> {
    message: MessageContent;
    /**
     * Optional chat history if you want to customize the chat history.
     */
    chatHistory?: ChatMessage<AdditionalMessageOptions>[] | BaseMemory<AdditionalMessageOptions>;
}
interface StreamingChatEngineParams<AdditionalMessageOptions extends object = object> extends BaseChatEngineParams<AdditionalMessageOptions> {
    stream: true;
}
interface NonStreamingChatEngineParams<AdditionalMessageOptions extends object = object> extends BaseChatEngineParams<AdditionalMessageOptions> {
    stream?: false;
}
declare abstract class BaseChatEngine {
    abstract chat(params: NonStreamingChatEngineParams): Promise<EngineResponse>;
    abstract chat(params: StreamingChatEngineParams): Promise<AsyncIterable<EngineResponse>>;
    abstract chatHistory: ChatMessage[] | Promise<ChatMessage[]>;
}

interface Context {
    message: ChatMessage;
    nodes: NodeWithScore[];
}
/**
 * A ContextGenerator is used to generate a context based on a message's text content
 */
interface ContextGenerator {
    generate(message: string): Promise<Context>;
}

/**
 * ContextChatEngine uses the Index to get the appropriate context for each query.
 * The context is stored in the system prompt, and the chat history is chunk: ChatResponseChunk, nodes?: NodeWithScore<import("/Users/marcus/code/llamaindex/LlamaIndexTS/packages/core/src/Node").Metadata>[], nodes?: NodeWithScore<import("/Users/marcus/code/llamaindex/LlamaIndexTS/packages/core/src/Node").Metadata>[]lowing the appropriate context to be surfaced for each query.
 */
declare class ContextChatEngine extends PromptMixin implements BaseChatEngine {
    chatModel: LLM;
    memory: BaseMemory;
    contextGenerator: ContextGenerator & PromptMixin;
    systemPrompt?: string | undefined;
    get chatHistory(): ChatMessage<object>[] | Promise<ChatMessage<object>[]>;
    constructor(init: {
        retriever: BaseRetriever;
        chatModel?: LLM | undefined;
        chatHistory?: ChatMessage[] | undefined;
        contextSystemPrompt?: ContextSystemPrompt | undefined;
        nodePostprocessors?: BaseNodePostprocessor[] | undefined;
        systemPrompt?: string | undefined;
        contextRole?: MessageType | undefined;
    });
    protected _getPrompts(): PromptsRecord;
    protected _updatePrompts(prompts: {
        contextSystemPrompt: ContextSystemPrompt;
    }): void;
    protected _getPromptModules(): ModuleRecord;
    chat(params: NonStreamingChatEngineParams): Promise<EngineResponse>;
    chat(params: StreamingChatEngineParams): Promise<AsyncIterable<EngineResponse>>;
    reset(): void;
    private prepareRequestMessages;
    private prependSystemPrompt;
}

declare class DefaultContextGenerator extends PromptMixin implements ContextGenerator {
    retriever: BaseRetriever;
    contextSystemPrompt: ContextSystemPrompt;
    nodePostprocessors: BaseNodePostprocessor[];
    contextRole: MessageType;
    metadataMode?: MetadataMode;
    constructor(init: {
        retriever: BaseRetriever;
        contextSystemPrompt?: ContextSystemPrompt | undefined;
        nodePostprocessors?: BaseNodePostprocessor[] | undefined;
        contextRole?: MessageType | undefined;
        metadataMode?: MetadataMode | undefined;
    });
    protected _getPromptModules(): ModuleRecord;
    protected _getPrompts(): {
        contextSystemPrompt: ContextSystemPrompt;
    };
    protected _updatePrompts(promptsDict: {
        contextSystemPrompt: ContextSystemPrompt;
    }): void;
    private applyNodePostprocessors;
    generate(message: MessageContent): Promise<Context>;
}

/**
 * SimpleChatEngine is the simplest possible chat engine. Useful for using your own custom prompts.
 */
declare class SimpleChatEngine implements BaseChatEngine {
    memory: BaseMemory;
    llm: LLM;
    get chatHistory(): ___llms.ChatMessage<object>[] | Promise<___llms.ChatMessage<object>[]>;
    constructor(init?: Partial<SimpleChatEngine>);
    chat(params: NonStreamingChatEngineParams): Promise<EngineResponse>;
    chat(params: StreamingChatEngineParams): Promise<AsyncIterable<EngineResponse>>;
    reset(): void;
}

export { BaseChatEngine, type BaseChatEngineParams, ContextChatEngine, DefaultContextGenerator, type NonStreamingChatEngineParams, SimpleChatEngine, type StreamingChatEngineParams };
