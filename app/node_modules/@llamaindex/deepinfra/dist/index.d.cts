import { BaseEmbedding } from '@llamaindex/core/embeddings';
import { MessageContentDetail } from '@llamaindex/core/llms';
import { OpenAI } from '@llamaindex/openai';

interface DeepInfraEmbeddingResponse {
    embeddings: number[][];
    request_id: string;
    inference_status: InferenceStatus;
}
interface InferenceStatus {
    status: string;
    runtime_ms: number;
    cost: number;
    tokens_input: number;
}
/**
 * DeepInfraEmbedding is an alias for DeepInfra that implements the BaseEmbedding interface.
 */
declare class DeepInfraEmbedding extends BaseEmbedding {
    /**
     * DeepInfra model to use
     * @default "sentence-transformers/clip-ViT-B-32"
     * @see https://deepinfra.com/models/embeddings
     */
    model: string;
    /**
     * DeepInfra API token
     * @see https://deepinfra.com/dash/api_keys
     * If not provided, it will try to get the token from the environment variable `DEEPINFRA_API_TOKEN`
     *
     */
    apiToken: string;
    /**
     * Prefix to add to the query
     * @default ""
     */
    queryPrefix: string;
    /**
     * Prefix to add to the text
     * @default ""
     */
    textPrefix: string;
    /**
     *
     * @default 5
     */
    maxRetries: number;
    /**
     *
     * @default 60 * 1000
     */
    timeout: number;
    constructor(init?: Partial<DeepInfraEmbedding>);
    getTextEmbedding(text: string): Promise<number[]>;
    getQueryEmbedding(query: MessageContentDetail): Promise<number[] | null>;
    getTextEmbeddings: (texts: string[]) => Promise<number[][]>;
    getQueryEmbeddings(queries: string[]): Promise<number[][]>;
    private getDeepInfraEmbedding;
    private getUrl;
}

declare class DeepInfra extends OpenAI {
    constructor(init?: Omit<Partial<OpenAI>, "session">);
}

export { DeepInfra, DeepInfraEmbedding, type DeepInfraEmbeddingResponse, type InferenceStatus };
