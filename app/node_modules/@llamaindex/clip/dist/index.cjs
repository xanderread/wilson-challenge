Object.defineProperty(exports, '__esModule', { value: true });

var embeddings = require('@llamaindex/core/embeddings');
var global = require('@llamaindex/core/global');
var env = require('@llamaindex/env');

var ClipEmbeddingModelType = /*#__PURE__*/ function(ClipEmbeddingModelType) {
    ClipEmbeddingModelType["XENOVA_CLIP_VIT_BASE_PATCH32"] = "Xenova/clip-vit-base-patch32";
    ClipEmbeddingModelType["XENOVA_CLIP_VIT_BASE_PATCH16"] = "Xenova/clip-vit-base-patch16";
    return ClipEmbeddingModelType;
}({});

async function readImage(input) {
    const { RawImage } = await env.loadTransformers((transformer)=>{
        global.Settings.callbackManager.dispatchEvent("load-transformers", {
            transformer
        }, true);
    });
    if (input instanceof Blob) {
        return await RawImage.fromBlob(input);
    } else if (typeof input === "string" || input instanceof URL) {
        return await RawImage.fromURL(input);
    } else {
        throw new Error(`Unsupported input type: ${typeof input}`);
    }
}
class ClipEmbedding extends embeddings.MultiModalEmbedding {
    constructor(){
        super(), this.modelType = ClipEmbeddingModelType.XENOVA_CLIP_VIT_BASE_PATCH16, this.tokenizer = null, this.processor = null, this.visionModel = null, this.textModel = null;
    }
    async getTokenizer() {
        const { AutoTokenizer } = await env.loadTransformers((transformer)=>{
            global.Settings.callbackManager.dispatchEvent("load-transformers", {
                transformer
            }, true);
        });
        if (!this.tokenizer) {
            this.tokenizer = await AutoTokenizer.from_pretrained(this.modelType);
        }
        return this.tokenizer;
    }
    async getProcessor() {
        const { AutoProcessor } = await env.loadTransformers((transformer)=>{
            global.Settings.callbackManager.dispatchEvent("load-transformers", {
                transformer
            }, true);
        });
        if (!this.processor) {
            this.processor = await AutoProcessor.from_pretrained(this.modelType);
        }
        return this.processor;
    }
    async getVisionModel() {
        const { CLIPVisionModelWithProjection } = await env.loadTransformers((transformer)=>{
            global.Settings.callbackManager.dispatchEvent("load-transformers", {
                transformer
            }, true);
        });
        if (!this.visionModel) {
            this.visionModel = await CLIPVisionModelWithProjection.from_pretrained(this.modelType);
        }
        return this.visionModel;
    }
    async getTextModel() {
        const { CLIPTextModelWithProjection } = await env.loadTransformers((transformer)=>{
            global.Settings.callbackManager.dispatchEvent("load-transformers", {
                transformer
            }, true);
        });
        if (!this.textModel) {
            this.textModel = await CLIPTextModelWithProjection.from_pretrained(this.modelType);
        }
        return this.textModel;
    }
    async getImageEmbedding(image) {
        const loadedImage = await readImage(image);
        const imageInputs = await (await this.getProcessor())(loadedImage);
        const { image_embeds } = await (await this.getVisionModel())(imageInputs);
        return Array.from(image_embeds.data);
    }
    async getTextEmbedding(text) {
        const textInputs = await (await this.getTokenizer())([
            text
        ], {
            padding: true,
            truncation: true
        });
        const { text_embeds } = await (await this.getTextModel())(textInputs);
        return text_embeds.data;
    }
}

exports.ClipEmbedding = ClipEmbedding;
exports.ClipEmbeddingModelType = ClipEmbeddingModelType;
