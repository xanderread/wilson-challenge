import { BaseLLM, LLMMetadata, LLMChatParamsStreaming, ChatResponseChunk, LLMChatParamsNonStreaming, ChatResponse, ChatMessage } from '@llamaindex/core/llms';
import { Portkey as Portkey$1 } from 'portkey-ai';

type PortkeyOptions = ConstructorParameters<typeof Portkey$1>[0];
declare class PortkeySession {
    portkey: Portkey$1;
    constructor(options?: PortkeyOptions);
}
/**
 * Get a session for the Portkey API. If one already exists with the same options,
 * it will be returned. Otherwise, a new session will be created.
 * @param options
 * @returns
 */
declare function getPortkeySession(options?: PortkeyOptions): PortkeySession;
declare class Portkey extends BaseLLM {
    apiKey?: string | undefined;
    baseURL?: string | undefined;
    session: PortkeySession;
    constructor(init?: Partial<Portkey> & PortkeyOptions);
    get metadata(): LLMMetadata;
    chat(params: LLMChatParamsStreaming): Promise<AsyncIterable<ChatResponseChunk>>;
    chat(params: LLMChatParamsNonStreaming): Promise<ChatResponse>;
    streamChat(messages: ChatMessage[], params?: Record<string, any>): AsyncIterable<ChatResponseChunk>;
}

export { Portkey, PortkeySession, getPortkeySession };
