import { BaseEmbedding } from '@llamaindex/core/embeddings';
import { LoadTransformerEvent } from '@llamaindex/env';
import { BaseLLM, LLMMetadata, LLMChatParamsStreaming, ChatResponseChunk, LLMChatParamsNonStreaming, ChatResponse } from '@llamaindex/core/llms';
import { PreTrainedTokenizer, PreTrainedModel } from '@xenova/transformers';
import { HfInference } from '@huggingface/inference';

declare module "@llamaindex/core/global" {
    interface LlamaIndexEventMaps {
        "load-transformers": LoadTransformerEvent;
    }
}
/**
 * Uses feature extraction from '@xenova/transformers' to generate embeddings.
 * Per default the model [XENOVA_ALL_MINILM_L6_V2](https://huggingface.co/Xenova/all-MiniLM-L6-v2) is used.
 *
 * Can be changed by setting the `modelType` parameter in the constructor, e.g.:
 * ```
 * new HuggingFaceEmbedding({
 *     modelType: HuggingFaceEmbeddingModelType.XENOVA_ALL_MPNET_BASE_V2,
 * });
 * ```
 *
 * @extends BaseEmbedding
 */
declare class HuggingFaceEmbedding extends BaseEmbedding {
    modelType: string;
    quantized: boolean;
    private extractor;
    constructor(init?: Partial<HuggingFaceEmbedding>);
    getExtractor(): Promise<any>;
    getTextEmbedding(text: string): Promise<number[]>;
}

interface HFLLMConfig {
    modelName?: string;
    tokenizerName?: string;
    temperature?: number;
    topP?: number;
    maxTokens?: number;
    contextWindow?: number;
}
declare class HuggingFaceLLM extends BaseLLM {
    modelName: string;
    tokenizerName: string;
    temperature: number;
    topP: number;
    maxTokens?: number | undefined;
    contextWindow: number;
    private tokenizer;
    private model;
    constructor(init?: HFLLMConfig);
    get metadata(): LLMMetadata;
    getTokenizer(): Promise<PreTrainedTokenizer>;
    getModel(): Promise<PreTrainedModel>;
    chat(params: LLMChatParamsStreaming): Promise<AsyncIterable<ChatResponseChunk>>;
    chat(params: LLMChatParamsNonStreaming): Promise<ChatResponse>;
    protected nonStreamChat(params: LLMChatParamsNonStreaming): Promise<ChatResponse>;
    protected streamChat(params: LLMChatParamsStreaming): AsyncIterable<ChatResponseChunk>;
}

declare enum HuggingFaceEmbeddingModelType {
    XENOVA_ALL_MINILM_L6_V2 = "Xenova/all-MiniLM-L6-v2",
    XENOVA_ALL_MPNET_BASE_V2 = "Xenova/all-mpnet-base-v2"
}
/**
 * Uses feature extraction from Hugging Face's Inference API to generate embeddings.
 *
 * Set the `model` and `accessToken` parameter in the constructor, e.g.:
 * ```
 * new HuggingFaceInferenceAPIEmbedding({
 *     model: HuggingFaceEmbeddingModelType.XENOVA_ALL_MPNET_BASE_V2,
 *     accessToken: "<your-access-token>"
 * });
 * ```
 *
 * @extends BaseEmbedding
 */
declare class HuggingFaceInferenceAPIEmbedding extends BaseEmbedding {
    model: string;
    hf: HfInference;
    constructor(init: HFConfig);
    getTextEmbedding(text: string): Promise<number[]>;
    getTextEmbeddings: (texts: string[]) => Promise<Array<number[]>>;
}
type HfInferenceOptions = ConstructorParameters<typeof HfInference>[1];
type HFConfig = Partial<typeof DEFAULT_PARAMS> & HfInferenceOptions & {
    model: string;
    accessToken: string;
    endpoint?: string;
};
declare const DEFAULT_PARAMS: {
    temperature: number;
    topP: number;
    maxTokens: undefined;
    contextWindow: number;
};
/**
    Wrapper on the Hugging Face's Inference API.
    API Docs: https://huggingface.co/docs/huggingface.js/inference/README
    List of tasks with models: huggingface.co/api/tasks

    Note that Conversational API is not yet supported by the Inference API.
    They recommend using the text generation API instead.
    See: https://github.com/huggingface/huggingface.js/issues/586#issuecomment-2024059308
 */
declare class HuggingFaceInferenceAPI extends BaseLLM {
    model: string;
    temperature: number;
    topP: number;
    maxTokens?: number | undefined;
    contextWindow: number;
    hf: HfInference;
    constructor(init: HFConfig);
    get metadata(): LLMMetadata;
    chat(params: LLMChatParamsStreaming): Promise<AsyncIterable<ChatResponseChunk>>;
    chat(params: LLMChatParamsNonStreaming): Promise<ChatResponse>;
    private messagesToPrompt;
    protected nonStreamChat(params: LLMChatParamsNonStreaming): Promise<ChatResponse>;
    protected streamChat(params: LLMChatParamsStreaming): AsyncIterable<ChatResponseChunk>;
}

export { type HFConfig, type HFLLMConfig, HuggingFaceEmbedding, HuggingFaceEmbeddingModelType, HuggingFaceInferenceAPI, HuggingFaceInferenceAPIEmbedding, HuggingFaceLLM };
