import { HfInference } from '@huggingface/inference';
import { BaseEmbedding } from '@llamaindex/core/embeddings';
import { BaseLLM, LLMMetadata, LLMChatParamsStreaming, ChatResponseChunk, LLMChatParamsNonStreaming, ChatResponse } from '@llamaindex/core/llms';

declare enum HuggingFaceEmbeddingModelType {
    XENOVA_ALL_MINILM_L6_V2 = "Xenova/all-MiniLM-L6-v2",
    XENOVA_ALL_MPNET_BASE_V2 = "Xenova/all-mpnet-base-v2"
}
/**
 * Uses feature extraction from Hugging Face's Inference API to generate embeddings.
 *
 * Set the `model` and `accessToken` parameter in the constructor, e.g.:
 * ```
 * new HuggingFaceInferenceAPIEmbedding({
 *     model: HuggingFaceEmbeddingModelType.XENOVA_ALL_MPNET_BASE_V2,
 *     accessToken: "<your-access-token>"
 * });
 * ```
 *
 * @extends BaseEmbedding
 */
declare class HuggingFaceInferenceAPIEmbedding extends BaseEmbedding {
    model: string;
    hf: HfInference;
    constructor(init: HFConfig);
    getTextEmbedding(text: string): Promise<number[]>;
    getTextEmbeddings: (texts: string[]) => Promise<Array<number[]>>;
}
type HfInferenceOptions = ConstructorParameters<typeof HfInference>[1];
type HFConfig = Partial<typeof DEFAULT_PARAMS> & HfInferenceOptions & {
    model: string;
    accessToken: string;
    endpoint?: string;
};
declare const DEFAULT_PARAMS: {
    temperature: number;
    topP: number;
    maxTokens: undefined;
    contextWindow: number;
};
/**
    Wrapper on the Hugging Face's Inference API.
    API Docs: https://huggingface.co/docs/huggingface.js/inference/README
    List of tasks with models: huggingface.co/api/tasks

    Note that Conversational API is not yet supported by the Inference API.
    They recommend using the text generation API instead.
    See: https://github.com/huggingface/huggingface.js/issues/586#issuecomment-2024059308
 */
declare class HuggingFaceInferenceAPI extends BaseLLM {
    model: string;
    temperature: number;
    topP: number;
    maxTokens?: number | undefined;
    contextWindow: number;
    hf: HfInference;
    constructor(init: HFConfig);
    get metadata(): LLMMetadata;
    chat(params: LLMChatParamsStreaming): Promise<AsyncIterable<ChatResponseChunk>>;
    chat(params: LLMChatParamsNonStreaming): Promise<ChatResponse>;
    private messagesToPrompt;
    protected nonStreamChat(params: LLMChatParamsNonStreaming): Promise<ChatResponse>;
    protected streamChat(params: LLMChatParamsStreaming): AsyncIterable<ChatResponseChunk>;
}

declare const HuggingFaceEmbedding: any;
declare const HuggingFaceLLM: any;

export { type HFConfig, HuggingFaceEmbedding, HuggingFaceEmbeddingModelType, HuggingFaceInferenceAPI, HuggingFaceInferenceAPIEmbedding, HuggingFaceLLM };
